{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-13T19:39:54.571815Z","iopub.execute_input":"2021-12-13T19:39:54.572378Z","iopub.status.idle":"2021-12-13T19:39:54.670694Z","shell.execute_reply.started":"2021-12-13T19:39:54.572323Z","shell.execute_reply":"2021-12-13T19:39:54.669722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization, Input\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import to_categorical\n\ntrain = pd.read_csv('../input/Kannada-MNIST/train.csv')\ndig_mnist = pd.read_csv('../input/Kannada-MNIST/Dig-MNIST.csv')\n\n\nX_train = train.iloc[:,1:].to_numpy(dtype=np.float32, copy=True)\ny_train = train.iloc[:,0].to_numpy()\ny_train = to_categorical(y_train, num_classes=10)\nX_train = X_train.reshape(-1, 28,28,1)\nX_train = X_train / 255\n\nX_test = dig_mnist.iloc[:,1:].to_numpy(dtype=np.float32, copy=True)\ny_test = dig_mnist.iloc[:, 0].to_numpy()\ny_test = to_categorical(y_test, num_classes=10)\n\nX_test = X_test.reshape(-1,28,28,1)\nX_test = X_test / 255\n\nX_train, X_val, y_train, y_val = train_test_split(np.concatenate((X_train, X_test)),\n                                                 np.concatenate((y_train, y_test)),\n                                                 test_size=0.1, random_state=1,shuffle=True)\n\n\none_image = X_train[46]\n\nplt.imshow(one_image.reshape(28,28), cmap='gray')\nprint(f'This is an image of {y_train[46]}')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:09:51.699645Z","iopub.execute_input":"2021-12-16T21:09:51.700052Z","iopub.status.idle":"2021-12-16T21:10:07.377904Z","shell.execute_reply.started":"2021-12-16T21:09:51.699945Z","shell.execute_reply":"2021-12-16T21:10:07.37684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#baseline model\nbase_model = Sequential()\nbase_model.add(Input(shape=(28,28,1)))\nbase_model.add(Dense(10, activation='softmax'))\n\nbase_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')\nhistory = base_model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nepochs = range(len(train_acc))\n\nplt.plot(epochs, train_acc,'o-g', label='Train')\nplt.plot(epochs, val_acc,'o-b', label='Val')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_loss, 'o-g', label='Train')\nplt.plot(epochs, val_loss, 'o-b', label='Val')\nplt.legend()\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:04:11.093044Z","iopub.execute_input":"2021-12-13T20:04:11.093421Z","iopub.status.idle":"2021-12-13T20:04:11.533103Z","shell.execute_reply.started":"2021-12-13T20:04:11.093374Z","shell.execute_reply":"2021-12-13T20:04:11.532182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#Simple CNN\n\n#reshape to image dim\nX_train = X_train.reshape(-1, 28, 28, 1)\nprint(X_train.shape)\nX_train.dtype\n\nsimple_model = Sequential()\nsimple_model.add(Input(shape=(28,28,1)))\nsimple_model.add(Conv2D(32, 5, padding='same', activation='relu')) \nsimple_model.add(MaxPool2D())\nsimple_model.add(Conv2D(64, 5, padding='same', activation='relu'))\nsimple_model.add(MaxPool2D())\nsimple_model.add(Flatten())\nsimple_model.add(Dense(100, activation='relu'))\nsimple_model.add(Dense(10, activation='softmax'))\n                 \nsimple_model.compile(loss='categorical_crossentropy', optimizer='adam',\n                    metrics=['accuracy'])\n                 \nhistory = simple_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n'''","metadata":{"execution":{"iopub.status.busy":"2021-12-14T19:50:32.481729Z","iopub.execute_input":"2021-12-14T19:50:32.482132Z","iopub.status.idle":"2021-12-14T19:59:28.542554Z","shell.execute_reply.started":"2021-12-14T19:50:32.482091Z","shell.execute_reply":"2021-12-14T19:59:28.541433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nepochs = range(len(train_acc))\n\nplt.plot(epochs, train_acc,'o-g', label='Train')\nplt.plot(epochs, val_acc,'o-b', label='Val')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_loss, 'o-g', label='Train')\nplt.plot(epochs, val_loss, 'o-b', label='Val')\nplt.title('Loss')\nplt.legend()\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-12-14T20:00:26.889689Z","iopub.execute_input":"2021-12-14T20:00:26.890072Z","iopub.status.idle":"2021-12-14T20:00:27.338332Z","shell.execute_reply.started":"2021-12-14T20:00:26.890031Z","shell.execute_reply":"2021-12-14T20:00:27.336033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n#build better model\n\ndef cnn_model():\n    inp = tf.keras.Input(shape=(28,28,1))\n    x1 = tf.keras.layers.Conv2D(128, (1,1), strides=(1,1), activation='relu')(inp)\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x3 = tf.keras.layers.Conv2D(128, (3,3), padding='same', strides=(1,1), activation='relu')(inp)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    x5 = tf.keras.layers.Conv2D(128, (5,5), padding='same', strides=(1,1), activation='relu')(inp)\n    x5 = tf.keras.layers.BatchNormalization()(x5)\n    \n    #take average of each set (x1, x3, x5)\n    averaged = tf.keras.layers.Average()([x1,x3,x5])\n    averaged = tf.keras.layers.Activation('relu')(averaged)\n    averaged = tf.keras.layers.BatchNormalization()(averaged)\n\n    #apply to average values\n    x = tf.keras.layers.Conv2D(128, (5,5), strides=(1,1), activation='relu')(averaged)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n\n    #second layer\n    x1 = tf.keras.layers.Conv2D(128, (1,1), strides=(1,1), activation='relu')(x)\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x3 = tf.keras.layers.Conv2D(128, (3,3), padding='same', strides=(1,1), activation='relu')(x)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    x5 = tf.keras.layers.Conv2D(128, (5,5), padding='same', strides=(1,1), activation='relu')(x)\n    x5 = tf.keras.layers.BatchNormalization()(x5)\n\n    averaged = tf.keras.layers.Average()([x1,x3,x5])\n    averaged = tf.keras.layers.Activation('relu')(averaged)\n    averaged = tf.keras.layers.BatchNormalization()(averaged)\n\n    x = tf.keras.layers.Conv2D(128, (3,3), strides=(1,1), activation='relu')(averaged)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n\n    #third layer\n    x1 = tf.keras.layers.Conv2D(256, (1,1), strides=(1,1), activation='relu')(x)\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x3 = tf.keras.layers.Conv2D(256, (3,3), padding='same', strides=(1,1), activation='relu')(x)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    x5 = tf.keras.layers.Conv2D(256, (5,5), padding='same', strides=(1,1), activation='relu')(x)\n    x5 = tf.keras.layers.BatchNormalization()(x5)\n\n    averaged = tf.keras.layers.Average()([x1,x3,x5])\n    averaged = tf.keras.layers.Activation('relu')(averaged)\n    averaged = tf.keras.layers.BatchNormalization()(averaged)\n\n    x = tf.keras.layers.Conv2D(256, (1,1), strides=(1,1), activation='relu')(averaged)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n\n    #fourth layer\n    x1 = tf.keras.layers.Conv2D(256, (1,1), strides=(1,1), activation='relu')(x)\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x3 = tf.keras.layers.Conv2D(256, (3,3), padding='same', strides=(1,1), activation='relu')(x)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    x5 = tf.keras.layers.Conv2D(256, (5,5), padding='same', strides=(1,1), activation='relu')(x)\n    x5 = tf.keras.layers.BatchNormalization()(x5)\n\n    averaged = tf.keras.layers.Average()([x1,x3,x5])\n    averaged = tf.keras.layers.Activation('relu')(averaged)\n    averaged = tf.keras.layers.BatchNormalization()(averaged)\n\n    x = tf.keras.layers.Conv2D(256, (1,1), strides=(1,1), activation='relu')(averaged)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n    \n    #cnn\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(256, activation=tf.nn.relu)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(128, activation=tf.nn.relu)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    \n    output = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x)\n\n    model = tf.keras.Model(inputs=inp, outputs=output)\n    return model\n\nBATCH_SIZE= 128\nmodel=cnn_model()\nopt = Adam(learning_rate = 0.001)\nes = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\nlr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.75)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T21:09:56.604489Z","iopub.execute_input":"2021-12-14T21:09:56.604827Z","iopub.status.idle":"2021-12-14T21:10:04.424824Z","shell.execute_reply.started":"2021-12-14T21:09:56.604737Z","shell.execute_reply":"2021-12-14T21:10:04.424101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#alter train data to produce more data\ntrain_aug = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n                                                  width_shift_range=0.2,\n                                                  height_shift_range=0.2,\n                                                  shear_range=0.1,\n                                                  zoom_range=0.2,\n                                                  horizontal_flip=False)\nvalid_aug = tf.keras.preprocessing.image.ImageDataGenerator()\n\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, callbacks=[lr_reduction, es], verbose=1,\n         validation_data=(X_val, y_val))\n\nmodel.fit_generator(train_aug.flow(X_train, y_train,batch_size=BATCH_SIZE),\n                    steps_per_epoch=10,\n                    validation_data=valid_aug.flow(X_val, y_val),\n                    validation_steps=50,\n                    epochs=5, verbose=1,\n                       callbacks=[lr_reduction, es])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T21:14:44.031573Z","iopub.execute_input":"2021-12-14T21:14:44.032237Z","iopub.status.idle":"2021-12-14T21:18:19.168465Z","shell.execute_reply.started":"2021-12-14T21:14:44.032196Z","shell.execute_reply":"2021-12-14T21:18:19.167778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nepochs = range(len(train_acc))\n\nplt.plot(epochs, train_acc,'o-g', label='Train')\nplt.plot(epochs, val_acc,'o-b', label='Val')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_loss, 'o-g', label='Train')\nplt.plot(epochs, val_loss, 'o-b', label='Val')\nplt.title('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:00:45.508138Z","iopub.execute_input":"2021-12-13T21:00:45.509395Z","iopub.status.idle":"2021-12-13T21:01:37.373029Z","shell.execute_reply.started":"2021-12-13T21:00:45.509299Z","shell.execute_reply":"2021-12-13T21:01:37.372418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make predictions on real data\nreal_test = pd.read_csv('../input/Kannada-MNIST/test.csv')\n\nreal_test = real_test.iloc[:,1:].to_numpy()\nreal_test = real_test / 255\nreal_test = real_test.reshape(-1, 28, 28, 1)\n\nsample_submission = pd.read_csv('../input/Kannada-MNIST/sample_submission.csv')\n\npredictions = model.predict(real_test)\npredictions = predictions.argmax(axis=1)\n\npredictions = predictions.astype(int).flatten()\npredictions = (LabelEncoder().fit_transform((predictions)))\nsample_submission['Label'] = predictions\nsample_submission.to_csv('Submission file', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T21:23:12.554179Z","iopub.execute_input":"2021-12-14T21:23:12.554882Z","iopub.status.idle":"2021-12-14T21:23:13.693734Z","shell.execute_reply.started":"2021-12-14T21:23:12.55484Z","shell.execute_reply":"2021-12-14T21:23:13.692896Z"},"trusted":true},"execution_count":null,"outputs":[]}]}